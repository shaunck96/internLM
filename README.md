# internLM
Customer Service Using InternLM

Batch Inferencing of transcriptions using InternLM parallelized across 2 GPUs using Ray 
